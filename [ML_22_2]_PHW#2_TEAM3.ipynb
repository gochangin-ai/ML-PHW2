{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bQP5iohmg0lB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pyclustering.cluster.clarans import clarans\n",
    "from pyclustering.utils import timedcall;\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataProcessing(df)  \n",
    "## remove column ( ocean_proximity ) from df_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VSIccl1Hg0lE"
   },
   "outputs": [],
   "source": [
    "def DataProcessing(df):\n",
    "\n",
    "    #Drop nan\n",
    "    df = df.dropna()\n",
    "    df_numeric = df.drop(['ocean_proximity'],axis=1)\n",
    "    df_cat = df[['ocean_proximity']] \n",
    "    \n",
    "\n",
    "    return df_numeric, df_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataScaling(df,scalers)\n",
    "## get encoder lists and encoding dataframe\n",
    "\n",
    "### parameters : df, {DataFrame} data frame that you want to scale\n",
    "###                       scalers, {Scaler} Scalers that you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rLSFZjMIg0lE"
   },
   "outputs": [],
   "source": [
    "def DataScaling(df, scalers):\n",
    "    df_scaled_list = []\n",
    "    \n",
    "    for i in scalers:\n",
    "        i.fit(df)\n",
    "        data_scaled = i.transform(df)\n",
    "        df_scaled_list.append(pd.DataFrame(data = data_scaled, columns=df.columns))\n",
    "    \n",
    "    return df_scaled_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DataEncoding(df,encoders)\n",
    "## get scaler lists and scaling dataframe\n",
    "\n",
    "### parameters : df, {DataFrame} data frame that you want to scale\n",
    "###                       scalers, {encoders} encoders that you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dKk86Yweg0lE"
   },
   "outputs": [],
   "source": [
    "def DataEncoding(df, encoders):\n",
    "    df_encoded_list = []\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_cat.values.ravel())\n",
    "    labels = encoder.transform(df_cat)\n",
    "    df_encoded_list.append((pd.DataFrame(data = labels, columns=df_cat.columns)).reset_index(drop= True))\n",
    "    \n",
    "    df_encoded_list.append(pd.get_dummies(df_cat).reset_index(drop= True))\n",
    "    \n",
    "        \n",
    "    return df_encoded_list, encoder.classes_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DfConcat(df_numeric_scaled, df_cat_encoded)\n",
    "## get encoded and scaled dataframe and concatenate with various mix\n",
    "\n",
    "### parameters : df_numeric_scaled, {DataFrame} data frame that scaled\n",
    "###                       df_cat_encoded, {DataFrame} encoders that encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dEf8V3x1g0lF"
   },
   "outputs": [],
   "source": [
    "def DfConcat(df_numeric_scaled, df_cat_encoded):\n",
    "    df_list = []\n",
    "    \n",
    "    for i in df_numeric_scaled:\n",
    "        for j in df_cat_encoded:\n",
    "            df_list.append(pd.concat([i,j],axis = 1))\n",
    "    \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clusterToIdx(clusters,ratio)\n",
    "## make index lists for CLARANS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KhzvLUTRWEUW"
   },
   "outputs": [],
   "source": [
    "def clusterToIdx(clusters,ratio):\n",
    "    idx_list = [-1 for i in range(ratio)]\n",
    "    idx = 0\n",
    "\n",
    "    for k in clusters:\n",
    "        for i in k:\n",
    "            idx_list[i] = idx\n",
    "        idx = idx + 1\n",
    "\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show_pairplot(df)\n",
    "## make pairplot of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kXfbvViRFMel"
   },
   "outputs": [],
   "source": [
    "def show_pairplot(df):\n",
    "    sns.pairplot(df, hue = 'cluster')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cal_corr(df)\n",
    "## calculate correlation of each columns \n",
    "## and return dataframe that selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "87t1RKYJCM11"
   },
   "outputs": [],
   "source": [
    "def cal_corr(df):\n",
    "    corr = df.corr()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(corr, annot=True, fmt = '.2f', cmap = 'Blues')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"pairs with correlation coefficient is above 0.8\")\n",
    "    s = corr.unstack()\n",
    "    s_df = pd.DataFrame(s[s<1].sort_values(ascending=False), columns=['corr']) #sort with excepting corr = 1\n",
    "    s_df2 = s_df[s_df['corr']>0.8].drop_duplicates()\n",
    "    print(s_df2)\n",
    "\n",
    "    s_dict = s_df.to_dict()\n",
    "    s_keys = s_dict['corr'].keys()\n",
    "    s_list = list(s_keys)\n",
    "    df_select = df[[s_list[0][0],s_list[0][1]]] #select two features with the strongest correlation each other\n",
    "\n",
    "    return df_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# purity_score(y_true, y_pred):\n",
    "## compute purity score \n",
    "\n",
    "\n",
    "# makeplot(title, y_list, x_list):\n",
    "## make plot for scores (elbow,  silhouette, purity ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) \n",
    "def makeplot(title, y_list, x_list):\n",
    "    plt.plot(x_list, y_list, label =title, marker = 'o')\n",
    "    plt.title(title)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# AutoML (df_list, models, ratio,DBSCAN_list)\n",
    "## train model & plot result\n",
    "\n",
    "### parameters : df_list, {DataFrame} data frame list that will be used\n",
    "###                       models, {list} models that will be used\n",
    "###                       ratio, sample size ratio for CLARANS\n",
    "###                       DBSCAN_list, EPS and min_sample for DBSCAN parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jUVT1k4jSaXc"
   },
   "outputs": [],
   "source": [
    "def AutoML(df_list, models, ratio,DBSCAN_list):\n",
    "  for i, df in enumerate(df_list):\n",
    "    print(df_list_type[i])\n",
    "\n",
    "    for j in models:\n",
    "      if str(j) == \"KMeans()\":\n",
    "        print(str(j))\n",
    "        KMeanslabel_list = [-1 for i in range(0, len(df_list)*len(n_cluster))]\n",
    "        idx = 0\n",
    "        kmeans_distance_tmp= []\n",
    "        kmeans_silhouette_tmp = []\n",
    "        kmeans_purity_tmp = []\n",
    "        for k in range(2, 11, 2):\n",
    "          print(\"K = \", k)\n",
    "          df_copy = df\n",
    "          kmeans = KMeans(n_clusters = k, random_state = 0)\n",
    "          kmeans.fit(df)\n",
    "          labels = kmeans.predict(df)\n",
    "         \n",
    "          \n",
    "          KMeanslabel_list[idx] = labels\n",
    "          \n",
    "          idx = idx + 1\n",
    "          df['cluster'] =labels\n",
    "\n",
    "          \n",
    "          show_pairplot(df)\n",
    "          \n",
    "          df_select = cal_corr(df) #calculate corr and select pair with largest corr\n",
    "          \n",
    "          kmeans.fit(df_select)\n",
    "          kmeans_distance_tmp.append(kmeans.inertia_)\n",
    "          kmeans_silhouette_tmp.append(silhouette_score(df,kmeans.labels_,metric='euclidean'))\n",
    "          kmeans_purity_tmp.append(purity_score(median_house_value['label'],kmeans.labels_))\n",
    "          df_select['cluster'] = kmeans.predict(df_select)\n",
    "          print(\"select two features with the strongest correlation each other\")\n",
    "          show_pairplot(df_select)\n",
    "          \n",
    "        kmeans_sumofDistance = kmeans_distance_tmp\n",
    "        kmeans_silhouette = kmeans_silhouette_tmp \n",
    "        kmeans_purity = kmeans_purity_tmp \n",
    "        makeplot(\"KMeans_distance\", kmeans_sumofDistance, n_cluster)\n",
    "        makeplot(\"KMeans_silhouette\", kmeans_silhouette, n_cluster)\n",
    "        makeplot(\"KMeans_Purity\", kmeans_purity, n_cluster)\n",
    "        print(\"----------\")\n",
    "      elif str(j) == \"GaussianMixture()\":\n",
    "        print(str(j))\n",
    "        GaussianMixturelabel_list = [-1 for i in range(0, len(df_list)*len(n_cluster))]\n",
    "        idx=0\n",
    "        gmm_silhouette = []\n",
    "        gmm_purity= []\n",
    "        for k in range(2, 11, 2):\n",
    "          print(\"K = \", k)\n",
    "          gmm = GaussianMixture(n_components= k, random_state= 0)\n",
    "          gmm.fit(df)\n",
    "          labels = gmm.predict(df)\n",
    "          gmm_silhouette.append(silhouette_score(df,labels,metric='euclidean'))\n",
    "          gmm_purity.append(purity_score(median_house_value['label'], labels))\n",
    "          GaussianMixturelabel_list[idx] = labels\n",
    "          idx= idx+1\n",
    "          df['cluster'] =labels\n",
    "          show_pairplot(df)\n",
    "\n",
    "          df_select = cal_corr(df)\n",
    "          gmm.fit(df_select)\n",
    "          df_select['cluster'] = gmm.predict(df_select)\n",
    "          print(\"selcet two feature by correlation\")\n",
    "          show_pairplot(df_select)\n",
    "        makeplot(\"GaussianMixture_silhouette\", gmm_silhouette, n_cluster)\n",
    "        makeplot(\"GaussianMixture_Purity\", gmm_purity, n_cluster)\n",
    "        print(\"----------\")\n",
    "        \n",
    "\n",
    "      elif str(j) == \"clarans()\":\n",
    "        print(str(j))\n",
    "        Clarance_list = [-1 for i in range(0, len(df_list)*len(n_cluster))]\n",
    "        idx=0\n",
    "        clarans_silhouette = []\n",
    "        clarans_purity = []\n",
    "        for k in range(2, 11, 2):\n",
    "          print(\"K = \", k)\n",
    "          clarans_instance = clarans(df.values.tolist(), k, 6, 4).process()\n",
    "          clusters = clarans_instance.get_clusters()\n",
    "          labels = clusterToIdx(clusters,ratio)\n",
    "          df['cluster'] = labels\n",
    "          clarans_silhouette.append(silhouette_score(df, labels, metric='euclidean'))\n",
    "          clarans_purity.append(purity_score(median_house_value['label'], labels))\n",
    "          show_pairplot(df)\n",
    "\n",
    "          df_select = cal_corr(df)\n",
    "          clarans_instance = clarans(df_select.values.tolist(), k, 6, 4).process()\n",
    "          clusters = clarans_instance.get_clusters()\n",
    "          labels = clusterToIdx(clusters,ratio)\n",
    "          df_select['cluster'] = labels\n",
    "          print(\"selcet two feature by correlation\")\n",
    "          show_pairplot(df_select)\n",
    "        makeplot(\"clarans_silhouette\", clarans_silhouette, n_cluster)\n",
    "        makeplot(\"clarans_Purity\", clarans_purity, n_cluster)\n",
    "        print(\"----------\")\n",
    "\n",
    "      elif str(j) == \"DBSCAN()\":\n",
    "        print(str(j))\n",
    "        DBSCAN_label_list = [-1 for i in range(0, len(df_list)*len(DBSCAN_list[\"eps\"])*len(DBSCAN_list[\"min_sample\"]))]\n",
    "        idx=0\n",
    "        dbscan_silhouette = []\n",
    "        dbscan_purity = []\n",
    "        for eps in DBSCAN_list[\"eps\"]:\n",
    "            max_silhouette = -2\n",
    "            max_purity = -2\n",
    "            for sample in DBSCAN_list[\"min_sample\"]:\n",
    "                print(\"eps = \", eps,\"sample = \",sample)\n",
    "                dbscan = DBSCAN(eps=eps,min_samples=sample)\n",
    "                labels = dbscan.fit_predict(df)\n",
    "                DBSCAN_label_list[idx] = labels\n",
    "                df['cluster'] = labels\n",
    "                show_pairplot(df)\n",
    "\n",
    "                df_select = cal_corr(df)\n",
    "                labels = dbscan.fit_predict(df_select)\n",
    "                df_select['cluster'] = labels\n",
    "                print(\"selcet two feature by correlation\")\n",
    "                show_pairplot(df_select)\n",
    "                try:\n",
    "                    current_silhouette = silhouette_score(df_select, labels, metric='euclidean')\n",
    "                except:\n",
    "                    current_silhouette = -5\n",
    "                if max_silhouette < current_silhouette:\n",
    "                    max_silhouette = current_silhouette\n",
    "                current_purity = purity_score(median_house_value['label'], labels)\n",
    "                if max_purity < current_purity:\n",
    "                    max_purity = current_purity\n",
    "            dbscan_silhouette.append(max_silhouette)\n",
    "            dbscan_purity.append(max_purity) \n",
    "        dbscan_xlist = []\n",
    "        for i in DBSCAN_list[\"eps\"]:\n",
    "            tmp_str = str(i)\n",
    "            dbscan_xlist.append(tmp_str)\n",
    "        makeplot(\"DBSCAN_silhouette\",dbscan_silhouette , dbscan_xlist)\n",
    "        makeplot(\"DBSCAN_purity\",dbscan_purity , dbscan_xlist)\n",
    "    \n",
    "\n",
    "                    \n",
    "        print(\"----------\")\n",
    "\n",
    "      print(\"==========\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main\n",
    "\n",
    "\n",
    "##### n_cluster   you can set how many clusters you will create by list\n",
    "##### DBSCAN_list  you can set  eps and min_samples of dbscan\n",
    "##### we are using callifornia housing dataset\n",
    "##### https://www.kaggle.com/datasets/camnugent/california-housing-prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wYZMJ8cg0lH",
    "outputId": "ce792cef-81fd-49d2-fe4a-52e00e931c62",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. Data Load & Missing Data check\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#main\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== 1. Data Load & Missing Data check\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhousing.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(ratio, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "\n",
    "\n",
    "print(\"=== 1. Data Load & Missing Data check\")\n",
    "df = pd.read_csv('housing.csv')\n",
    "\n",
    "ratio = int(len(df) / 100 * 1.5)\n",
    "df = df.sample(ratio, random_state=42)\n",
    "print(df)\n",
    "\n",
    "#hyper parameter\n",
    "n_cluster = list(range(2, 11, 2))\n",
    "DBSCAN_list = {'eps': [0.1, 0.2, 0.5, 5, 10, 100, 1000], 'min_sample': [10, 20]}\n",
    "\n",
    "\n",
    "print(\"=== 2. split median_house_value & labeling\")\n",
    "median_house_value = pd.DataFrame(df[\"median_house_value\"])\n",
    "df = df.drop(columns=[\"median_house_value\"])\n",
    "\n",
    "bins = list(range(14998, 500002, 48500))\n",
    "median_house_value['label'] = pd.cut(median_house_value[\"median_house_value\"], \n",
    "                                bins, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "# print(median_house_value.groupby('label')['median_house_value'].apply(my_summary).unstack())\n",
    "\n",
    "print(\"=== 3. drop not use data (total_bedrooms \")\n",
    "dataset = df.drop(columns=[\"total_bedrooms\"])\n",
    "\n",
    "print(\"=== 4. Preprocessing\")\n",
    "df_numeric, df_cat = DataProcessing(dataset)\n",
    "\n",
    "scalers = [StandardScaler(),MinMaxScaler(), RobustScaler()]\n",
    "encoders = ['LabelEncoder', 'OneHotEncoder']\n",
    "\n",
    "df_numeric_scaled = DataScaling(df_numeric, scalers)\n",
    "df_cat_encoded, labels = DataEncoding(df_cat, encoders)\n",
    "\n",
    "df_list = DfConcat(df_numeric_scaled, df_cat_encoded)\n",
    "\n",
    "print(\"=== 4. Clustering & Evaluation\")\n",
    "df_list_type = ['SS & LE', 'SS & d', 'MMS & LE', 'MMS & d','RS & LE', 'RS & d' ]\n",
    "models = ['KMeans()','GaussianMixture()','clarans()','DBSCAN()']\n",
    "\n",
    "AutoML(df_list, models, ratio, DBSCAN_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4G1XKB8Gg0lI",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
